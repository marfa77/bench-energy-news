"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —É–≥–ª—é —á–µ—Ä–µ–∑ Gemini API —Å Google Search –∏–ª–∏ Discovery Engine.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç REST API –Ω–∞–ø—Ä—è–º—É—é (–∫–∞–∫ –≤ Dubai RE Soft Launch).
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Discovery Engine –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º.
"""
import os
import json
import time
import requests
from datetime import datetime
from typing import List, Dict, Optional
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


def _search_via_discovery_engine(query: str, max_results: int = 10) -> List[Dict]:
    """
    –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Discovery Engine API (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Service Account JSON –¥–ª—è OAuth2 –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –µ—Å–ª–∏ Discovery Engine –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
    """
    project_id = os.getenv("DISCOVERY_ENGINE_PROJECT_ID")
    location = os.getenv("DISCOVERY_ENGINE_LOCATION", "global")
    data_store_id = os.getenv("DISCOVERY_ENGINE_DATA_STORE_ID")
    serving_config_id = os.getenv("DISCOVERY_ENGINE_SERVING_CONFIG_ID", "default_search")
    
    # –ü—É—Ç—å –∫ Service Account JSON —Ñ–∞–π–ª—É
    service_account_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    if not service_account_path:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
        from pathlib import Path
        project_root = Path(__file__).parent
        possible_paths = [
            project_root / "becnh-482911-03729a4482e4.json",
            project_root / "service-account.json",
            project_root / "credentials.json"
        ]
        for path in possible_paths:
            if path.exists():
                service_account_path = str(path)
                break
    
    if not all([project_id, data_store_id]):
        return []  # Discovery Engine –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω
    
    if not service_account_path or not os.path.exists(service_account_path):
        print("‚ö†Ô∏è  Service Account JSON –Ω–µ –Ω–∞–π–¥–µ–Ω. Discovery Engine —Ç—Ä–µ–±—É–µ—Ç OAuth2 —Ç–æ–∫–µ–Ω.")
        return []
    
    try:
        # Discovery Engine Search API endpoint
        url = f"https://discoveryengine.googleapis.com/v1/projects/{project_id}/locations/{location}/dataStores/{data_store_id}/servingConfigs/{serving_config_id}:search"
        
        # –ü–æ–ª—É—á–∞–µ–º OAuth2 —Ç–æ–∫–µ–Ω –∏–∑ Service Account JSON
        try:
            from google.oauth2 import service_account
            from google.auth.transport.requests import Request
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º credentials –∏–∑ JSON —Ñ–∞–π–ª–∞
            credentials = service_account.Credentials.from_service_account_file(
                service_account_path,
                scopes=['https://www.googleapis.com/auth/cloud-platform']
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if not credentials.valid:
                credentials.refresh(Request())
            
            access_token = credentials.token
            
        except ImportError:
            print("‚ö†Ô∏è  –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ google-auth –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install google-auth")
            return []
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è OAuth2 —Ç–æ–∫–µ–Ω–∞: {e}")
            # Fallback: –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ gcloud
            import subprocess
            try:
                result = subprocess.run(
                    ["gcloud", "auth", "print-access-token"],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    access_token = result.stdout.strip()
                else:
                    print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å OAuth2 —Ç–æ–∫–µ–Ω")
                    return []
            except:
                print("‚ö†Ô∏è  gcloud –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ Service Account –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
                return []
        
        headers = {
            "Authorization": f"Bearer {access_token}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "query": query,
            "pageSize": max_results,
            "queryExpansionSpec": {
                "condition": "AUTO"
            },
            "spellCorrectionSpec": {
                "mode": "AUTO"
            }
        }
        
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Discovery Engine
        news_list = []
        if "results" in data:
            for result in data["results"]:
                document = result.get("document", {})
                struct_data = document.get("structData", {})
                
                title = struct_data.get("title") or document.get("title", "")
                url = struct_data.get("link") or document.get("id", "")
                snippet = struct_data.get("snippet") or struct_data.get("htmlSnippet", "")
                
                if title and url:
                    news_list.append({
                        "title": title,
                        "summary": snippet[:500] if snippet else "",
                        "source_name": url.split("/")[2] if "/" in url else "Unknown",
                        "source_url": url,
                        "publication_date": None
                    })
        
        return news_list
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ Discovery Engine: {e}")
        return []


def search_coal_news(max_retries: int = 3) -> List[Dict]:
    """
    –ò—â–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ —É–≥–ª—é –∑–∞ —Å–µ–≥–æ–¥–Ω—è —á–µ—Ä–µ–∑ Discovery Engine –∏–ª–∏ Gemini —Å Google Search.
    –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Discovery Engine (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω), –∑–∞—Ç–µ–º fallback –Ω–∞ Google Search.
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏, –∫–∞–∂–¥–∞—è –Ω–æ–≤–æ—Å—Ç—å —Å–æ–¥–µ—Ä–∂–∏—Ç:
        - title: –∑–∞–≥–æ–ª–æ–≤–æ–∫
        - summary: –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
        - source_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        - source_url: URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        - publication_date: publication date
        
    Raises:
        Exception: –ï—Å–ª–∏ –ø–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫
    """
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise ValueError("GEMINI_API_KEY not set in environment")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Å—Ç—Ä–æ–µ–Ω –ª–∏ Discovery Engine
    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å project_id –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏–∑ Service Account JSON
    project_id = os.getenv("DISCOVERY_ENGINE_PROJECT_ID")
    if not project_id:
        # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–∑ Service Account JSON
        from pathlib import Path
        service_account_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        if not service_account_path:
            possible_paths = [
                Path(__file__).parent / "becnh-482911-03729a4482e4.json",
                Path(__file__).parent / "service-account.json",
                Path(__file__).parent / "credentials.json"
            ]
            for path in possible_paths:
                if path.exists():
                    service_account_path = str(path)
                    break
        
        if service_account_path and os.path.exists(service_account_path):
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç json –∏–∑ –Ω–∞—á–∞–ª–∞ —Ñ–∞–π–ª–∞
                with open(service_account_path, 'r') as f:
                    sa_data = json.load(f)
                    project_id = sa_data.get('project_id')
                    if project_id:
                        print(f"   üìã Project ID –∏–∑ Service Account: {project_id}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å project_id –∏–∑ Service Account: {e}")
    
    use_discovery_engine = all([
        project_id,
        os.getenv("DISCOVERY_ENGINE_DATA_STORE_ID")
    ])
    
    if use_discovery_engine:
        print("üîç –ò—Å–ø–æ–ª—å–∑—É—é Discovery Engine –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π...")
        # –ü—Ä–æ–±—É–µ–º –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Discovery Engine
        today_str = datetime.now().strftime("%Y-%m-%d")
        queries = [
            f"coal prices Australia FOB Newcastle {today_str}",
            f"coal prices Europe CIF ARA {today_str}",
            f"thermal coal market news {today_str}",
            f"coking coal prices {today_str}",
            f"coal export Indonesia {today_str}",
            f"coal export Australia {today_str}"
        ]
        
        all_news = []
        for query in queries:
            news = _search_via_discovery_engine(query, max_results=5)
            all_news.extend(news)
            time.sleep(0.5)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        if all_news:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_news)} –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Discovery Engine")
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —á–µ—Ä–µ–∑ Gemini –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è summary
            return _process_discovery_engine_results(all_news)
        else:
            print("‚ö†Ô∏è  Discovery Engine –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ Google Search...")
    
    # Fallback –Ω–∞ Google Search —á–µ—Ä–µ–∑ Gemini
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º REST API –Ω–∞–ø—Ä—è–º—É—é (–∫–∞–∫ –≤ Dubai RE Soft Launch)
    # –≠—Ç–æ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Vertex AI –ø—Ä–æ–µ–∫—Ç–∞
    
    today = datetime.now()
    today_str = today.strftime("%Y-%m-%d")
    today_full = today.strftime("%B %d, %Y")
    
    system_instruction = """You are a professional coal market news analyst with access to Google Search. Your task is to find the FRESHEST, MOST IMPORTANT AND HIGH-QUALITY coal market news.

CRITICAL SEARCH RULES (MUST FOLLOW):
1. MUST use google_search tool for searching
2. Make MANY DIFFERENT search queries (minimum 10-12), using SPECIFIC terms:
   - "coal prices Australia FOB Newcastle [date]"
   - "coal prices Europe CIF ARA [date]"
   - "thermal coal market news [date]"
   - "coking coal prices [date]"
   - "coal export Indonesia [date]"
   - "coal export Australia [date]"
   - "coal mining production [date]"
   - "coal freight rates [date]"
   - "coal demand China India [date]"
   - "coal supply disruptions [date]"
   - "coal policy regulation [date]"
   - "coal benchmark prices [date]"
3. PRIORITY SOURCES (search first):
   - Reuters, Bloomberg, Financial Times
   - Argus Media, Platts, S&P Global Commodity Insights
   - Trade publications (Hellenic Shipping News, TradeWinds)
   - Industry news sites (GMK Center, Mysteel, Petromindo)
   - Government and regulatory sources
4. SEARCH FOR SPECIFIC NEWS, not general articles:
   - News about specific prices, exports, imports
   - News about specific companies, projects, deals
   - News about policy changes, regulations
   - DO NOT search for general articles about "commodities" or "energy markets"
5. Use ONLY data from found sources - DO NOT invent anything
6. NEVER create news from your head - only from real sources
7. Check source URLs - they must be real and accessible
8. Check publication date - news must be FRESH (today or yesterday, maximum 2 days ago)
9. PRIORITY: freshest, most significant and SPECIFIC coal news

STRICT LIMITATIONS:
- If there are NO real news in search results - return {"news": []}
- DO NOT invent headlines, numbers, events, dates
- DO NOT use general knowledge - ONLY found sources
- DO NOT create "perfect" news - real news may be incomplete
- If you doubt the reality of news - DO NOT include it

PRIORITY SOURCES (search first):
- Reuters, Bloomberg, Financial Times (highest priority)
- Argus Media, Platts, S&P Global Commodity Insights (highest priority)
- Trade publications: Hellenic Shipping News, TradeWinds, Lloyd's List
- Industry news: GMK Center, Mysteel, Petromindo, CoalMint
- Government and regulatory sources
- Regional news outlets (Australia, Indonesia, China, India, Europe)

AVOID:
- General news about "commodities" or "energy markets" without coal specifics
- News about politics/elections, unless directly about coal
- News from website sections (need specific articles)

FORBIDDEN:
- Inventing headlines, numbers, events
- Using information not from sources
- Creating news based on general knowledge"""
    
    prompt = f"""Today is {today_full}. Find the FRESHEST, MOST IMPORTANT AND HIGH-QUALITY coal market news announced in the last 24-48 hours.

MUST USE GOOGLE SEARCH FOR DEEP SEARCH:
- Use google_search tool to search the ENTIRE INTERNET
- Make MANY DIFFERENT SPECIFIC search queries (minimum 10-12):
  1. "coal prices Australia FOB Newcastle {today_str}"
  2. "coal prices Europe CIF ARA {today_str}"
  3. "thermal coal market news {today_str}"
  4. "coking coal prices {today_str}"
  5. "coal export Indonesia {today_str}"
  6. "coal export Australia {today_str}"
  7. "coal mining production news {today_str}"
  8. "coal freight rates shipping {today_str}"
  9. "coal demand China India {today_str}"
  10. "coal supply disruptions {today_str}"
  11. "coal policy regulation {today_str}"
  12. "coal benchmark prices API2 API4 {today_str}"
  
SOURCE PRIORITY:
- Search first: Reuters, Bloomberg, Argus Media, Platts, S&P Global
- Then: Trade publications, industry news sites
- Avoid general news about "commodities" without coal specifics
  
- Search the ENTIRE INTERNET - don't limit to major sources only
- Use ONLY data from found sources
- NEVER invent news
- Priority: freshest and most significant news

Search topics (SPECIFIC news):
1. Coal prices (thermal coal, coking coal) - specific numbers, indices, benchmarks
2. Coal export/import - specific volumes, countries, ports (Australia, Indonesia, China, India, Europe)
3. Coal mining and production - specific companies, projects, volumes
4. Coal freight and logistics - specific rates, routes, ports
5. Policy and regulation - specific decisions, rule changes
6. Demand and supply - specific numbers, forecasts, changes
7. Deals and contracts - specific companies, volumes, prices
8. Infrastructure - specific projects, ports, terminals

AVOID:
- General articles about "commodities" or "energy markets"
- News about politics/elections without coal connection
- Theoretical articles without specific facts

For EACH REAL news from sources (ONLY from found sources):
- title: EXACT headline from source (copy verbatim)
- summary: REAL content from source (2-3 sentences, MUST include specific numbers, prices, volumes, percentages, dates - NO vague phrases like "limited activity" or "not mentioned")
- source_name: real source name (any valid source from internet)
- source_url: REAL URL of SPECIFIC ARTICLE from search (NOT website section, but specific article with full URL, e.g. https://www.reuters.com/business/energy/coal-prices-rise-2024-01-15/)
- publication_date: "{today_str}" ONLY if event happened today and it's mentioned in source, otherwise null

CRITICAL REQUIREMENTS FOR SUMMARY:
- MUST include specific numbers: prices (USD/ton), volumes (million tons), percentages (%), dates
- MUST include concrete facts: company names, port names, specific countries, exact figures
- DO NOT use vague phrases: "limited activity", "not mentioned", "under observation", "no significant", "minimal", "expected", "likely"
- If article doesn't have specific numbers - DO NOT include this news (skip it)
- Summary must be at least 100 characters and contain real data

IMPORTANT: source_url must be URL of SPECIFIC ARTICLE, not website section (not /business/energy/, but full article URL)

Return ONLY JSON:
{{
    "news": [{{
        "title": "REAL headline from source (verbatim)",
        "summary": "REAL content from source (facts from article)",
        "source_name": "Source name",
        "source_url": "https://full-url-of-specific-article-from-search",
        "publication_date": "{today_str}" or null
    }}]
}}

CRITICALLY IMPORTANT FOR source_url:
- Must be URL of SPECIFIC ARTICLE, not website section
- Examples of CORRECT URLs:
  ‚úÖ https://www.reuters.com/business/energy/coal-prices-rise-asia-2024-01-15/
  ‚úÖ https://www.bloomberg.com/news/articles/2024-01-15/indonesia-coal-exports
  ‚úÖ https://www.ft.com/content/abc123def456
- Examples of INCORRECT URLs (DO NOT use):
  ‚ùå https://www.reuters.com/business/energy/
  ‚ùå https://www.bloomberg.com/news/energy
  ‚ùå https://www.ft.com/energy
- If search results only show sections - DO NOT include such news

CRITICALLY IMPORTANT:
- If there are NO real news in sources with SPECIFIC DATA (numbers, prices, volumes) - return {{"news": []}}
- DO NOT invent news - ONLY from search
- DO NOT use general knowledge - ONLY found sources
- DO NOT include news without specific numbers, prices, or concrete facts
- Check URLs - they must be real and accessible
- If you doubt the reality of news - DO NOT include it
- If news looks "too perfect" or "too complete" without source - it's a sign of invented news
- If news has only vague phrases without numbers - DO NOT include it
- Better return empty array than vague or invented news"""
    
    for attempt in range(max_retries):
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º REST API –Ω–∞–ø—Ä—è–º—É—é (–∫–∞–∫ –≤ Dubai RE Soft Launch)
            # –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Vertex AI –ø—Ä–æ–µ–∫—Ç–∞
            model_name = "gemini-2.0-flash-exp"
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"
            
            payload = {
                "systemInstruction": {
                    "parts": [{"text": system_instruction}]
                },
                "contents": [{
                    "parts": [{"text": prompt}]
                }],
                "generationConfig": {
                    "temperature": 0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    "topK": 1,
                    "topP": 0.1
                },
                "tools": [{
                    "googleSearch": {}
                }]
            }
            headers = {"Content-Type": "application/json"}
            
            print(f"   –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å –∫ Gemini API (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 30-60 —Å–µ–∫—É–Ω–¥)...")
            response = requests.post(url, json=payload, headers=headers, timeout=90)
            response.raise_for_status()
            data = response.json()
            print(f"   ‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Gemini API")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            if 'candidates' in data and len(data['candidates']) > 0:
                candidate = data['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    response_text = candidate['content']['parts'][0].get('text', '')
                else:
                    response_text = str(data)
            else:
                response_text = str(data)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–∏—Å–∫ —Å—Ä–∞–±–æ—Ç–∞–ª
            if 'candidates' in data and len(data['candidates']) > 0:
                candidate = data['candidates'][0]
                if 'groundingMetadata' in candidate:
                    print("‚úÖ Google Search –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                    if 'searchEntryPoint' in candidate['groundingMetadata']:
                        print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {candidate['groundingMetadata']['searchEntryPoint']}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            response_text = response_text.strip()
            
            # –£–±–∏—Ä–∞–µ–º markdown –∫–æ–¥ –±–ª–æ–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
            if response_text.startswith("```json"):
                response_text = response_text[7:]
            if response_text.startswith("```"):
                response_text = response_text[3:]
            if response_text.endswith("```"):
                response_text = response_text[:-3]
            response_text = response_text.strip()
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            
            if json_start >= 0 and json_end > json_start:
                json_text = response_text[json_start:json_end]
            else:
                json_text = response_text
            
            # –ü–∞—Ä—Å–∏–º JSON –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞
            parsed_data = json.loads(json_text)
            news_list = parsed_data.get("news", [])
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ URL –∏–∑ groundingMetadata/citations (Gemini –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç "–∑–∞–∑–µ–º–ª–µ–Ω–Ω—ã–µ" —Å—Å—ã–ª–∫–∏)
            citations_map = {}  # –ú–∞–ø–ø–∏–Ω–≥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤/—Ç–µ–∫—Å—Ç–∞ –∫ —Ä–µ–∞–ª—å–Ω—ã–º URL
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π data –∏–∑ response.json() –¥–ª—è citations
            if 'candidates' in data and len(data['candidates']) > 0:
                candidate = data['candidates'][0]
                if 'groundingMetadata' in candidate:
                    grounding = candidate.get('groundingMetadata', {})
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º citations (—Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ URL)
                    if 'groundingChunks' in grounding:
                        for chunk in grounding.get('groundingChunks', []):
                            if 'web' in chunk:
                                web_data = chunk.get('web', {})
                                uri = web_data.get('uri', '')
                                title = web_data.get('title', '')
                                if uri and uri not in citations_map.values():
                                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º title –∫–∞–∫ –∫–ª—é—á –¥–ª—è –º–∞–ø–ø–∏–Ω–≥–∞
                                    if title:
                                        citations_map[title.lower()] = uri
                                    # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ –¥–æ–º–µ–Ω—É
                                    from urllib.parse import urlparse
                                    domain = urlparse(uri).netloc
                                    if domain:
                                        citations_map[domain.lower()] = uri
                    
                    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º searchEntryPoint –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö URL
                    if 'searchEntryPoint' in grounding:
                        entry_point = grounding.get('searchEntryPoint', {})
                        rendered_content = entry_point.get('renderedContent', '')
                        # –ú–æ–∂–µ–º –∏–∑–≤–ª–µ—á—å URL –∏–∑ renderedContent –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã - –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç–∏ —Ä–µ–∞–ª—å–Ω—ã–µ (–Ω–æ –ø—Ä–∏–Ω–∏–º–∞–µ–º –ª—é–±—ã–µ –≤–∞–ª–∏–¥–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)
            valid_news = []
            for item in news_list:
                if isinstance(item, dict) and "title" in item and "summary" in item:
                    source_url = item.get("source_url", "")
                    source_name = item.get("source_name", "Unknown")
                    title = item.get("title", "")
                    
                    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ä–µ–∞–ª—å–Ω—ã–π URL –≤ citations (–µ—Å–ª–∏ Gemini –≤–µ—Ä–Ω—É–ª "–∑–∞–∑–µ–º–ª–µ–Ω–Ω—ã–µ" —Å—Å—ã–ª–∫–∏)
                    if source_url and "vertexaisearch.cloud.google.com/grounding-api-redirect" in source_url:
                        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ citations –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É –∏–ª–∏ –¥–æ–º–µ–Ω—É
                        title_lower = title.lower()
                        source_name_lower = source_name.lower()
                        
                        # –ò—â–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                        for key, real_url in citations_map.items():
                            if key in title_lower or key in source_name_lower:
                                source_url = real_url
                                print(f"   üîó –ù–∞–π–¥–µ–Ω —Ä–µ–∞–ª—å–Ω—ã–π URL –∏–∑ citations: {real_url[:60]}...")
                                break
                        
                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ citations, –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç
                        if "vertexaisearch.cloud.google.com/grounding-api-redirect" in source_url:
                            try:
                                head_response = requests.head(source_url, allow_redirects=True, timeout=5)
                                if head_response.url and head_response.url != source_url:
                                    real_url = head_response.url
                                    print(f"   üîó –†–∞–∑–≤–µ—Ä–Ω—É—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç: {real_url[:60]}...")
                                    source_url = real_url
                            except Exception as e:
                                print(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—å —Ä–µ–¥–∏—Ä–µ–∫—Ç: {e}")
                                pass
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ URL –≤—ã–≥–ª—è–¥–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–º
                    if source_url and (source_url.startswith("http://") or source_url.startswith("https://")):
                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ —Ñ–µ–π–∫–æ–≤—ã–µ/—Ç–µ—Å—Ç–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                        invalid_patterns = [
                            "example.com", "test.com", "localhost", "127.0.0.1",
                            "placeholder", "dummy", "fake", "mock", "none", "null"
                        ]
                        url_lower = source_url.lower()
                        source_lower = source_name.lower()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ñ–µ–π–∫–æ–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
                        is_fake = any(pattern in url_lower or pattern in source_lower for pattern in invalid_patterns)
                        
                        if not is_fake:
                            # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –í–∞–ª–∏–¥–∏—Ä—É–µ–º URL (–ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–Ω –¥–æ—Å—Ç—É–ø–µ–Ω)
                            # –ù–æ –¥–µ–ª–∞–µ–º —ç—Ç–æ –±—ã—Å—Ç—Ä–æ, —á—Ç–æ–±—ã –Ω–µ –∑–∞–º–µ–¥–ª—è—Ç—å –ø–æ–∏—Å–∫
                            try:
                                from url_validator import validate_news_url
                                is_valid, error_msg = validate_news_url(source_url, timeout=5)
                                if is_valid:
                                    valid_news.append({
                                        "title": item.get("title", ""),
                                        "summary": item.get("summary", ""),
                                        "source_name": source_name,
                                        "source_url": source_url,
                                        "publication_date": item.get("publication_date", today_str)
                                    })
                                else:
                                    print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å —Å –±–∏—Ç–æ–π —Å—Å—ã–ª–∫–æ–π: {source_name} ({source_url[:50]}...) - {error_msg}")
                            except Exception as e:
                                # –ï—Å–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å (–±–µ–∑–æ–ø–∞—Å–Ω–µ–µ)
                                print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å URL {source_url[:50]}...: {e}")
                                print(f"   ‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å (–±–µ–∑–æ–ø–∞—Å–Ω–µ–µ –Ω–µ –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏)")
                        else:
                            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å –∏–∑ —Ñ–µ–π–∫–æ–≤–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {source_name} ({source_url[:50]}...)")
                    else:
                        print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å –±–µ–∑ –≤–∞–ª–∏–¥–Ω–æ–≥–æ URL: {item.get('title', '')[:50]}")
            
            if len(valid_news) == 0 and attempt == max_retries - 1:
                print(f"‚ö†Ô∏è  –ù–æ–≤–æ—Å—Ç–µ–π –ø–æ —É–≥–ª—é –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24-48 —á–∞—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            
            if valid_news:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(valid_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —É–≥–ª—é —á–µ—Ä–µ–∑ Google Search")
            else:
                print(f"‚ÑπÔ∏è  –ù–æ–≤–æ—Å—Ç–∏ –ø–æ —É–≥–ª—é –Ω–µ –Ω–∞–π–¥–µ–Ω—ã —á–µ—Ä–µ–∑ Google Search")
            
            return valid_news
            
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON. –û—Ç–≤–µ—Ç: {response_text[:200] if 'response_text' in locals() else 'N/A'}")
                return []
        except Exception as e:
            error_str = str(e).lower()
            error_full = str(e)
            
            if attempt == 0:
                print(f"üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ—à–∏–±–∫–∏: {error_full[:300]}")
            
            if "search tool" in error_str or "google_search" in error_str or "not supported" in error_str or "unknown field" in error_str:
                print(f"‚ö†Ô∏è  Google Search –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω!")
                print(f"üìã –í–∫–ª—é—á–∏—Ç–µ API 'Vertex AI Search and Conversation' –≤ Google Cloud Console:")
                print(f"   https://console.cloud.google.com/apis/library/discoveryengine.googleapis.com?project={os.getenv('VERTEX_AI_PROJECT_ID', 'your-project')}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    print(f"   –í–æ–∑–≤—Ä–∞—â–∞—é –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ (–Ω–µ –º–æ–≥—É –∏—Å–∫–∞—Ç—å –±–µ–∑ Google Search)")
                    return []
            
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                raise Exception(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {e}") from e
    
    return []


def select_best_news(news_list: List[Dict]) -> Optional[Dict]:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç —Å–∞–º—É—é —Ç–æ–ø–æ–≤—É—é –Ω–æ–≤–æ—Å—Ç—å –∏–∑ —Å–ø–∏—Å–∫–∞.
    
    –ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞:
    1. –°–≤–µ–∂–µ—Å—Ç—å (—Å–µ–≥–æ–¥–Ω—è > –≤—á–µ—Ä–∞ > –ø–æ–∑–∞–≤—á–µ—Ä–∞)
    2. –ó–Ω–∞—á–∏–º–æ—Å—Ç—å (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –≤–∞–∂–Ω–æ—Å—Ç—å —Ç–µ–º—ã)
    3. –ö–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–Ω–∞–¥–µ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤—ã—à–µ)
    4. –ù–∞–ª–∏—á–∏–µ URL –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    5. –î–ª–∏–Ω–∞ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å summary
    
    Args:
        news_list: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
        
    Returns:
        –°–∞–º–∞—è —Ç–æ–ø–æ–≤–∞—è –Ω–æ–≤–æ—Å—Ç—å –∏–ª–∏ None –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç–æ–π
    """
    if not news_list:
        return None
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å –≤–∞–ª–∏–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏ –ö–û–ù–ö–†–ï–¢–ù–´–ú–ò —Ñ–∞–∫—Ç–∞–º–∏
    import re
    valid_news = []
    for n in news_list:
        if not (n.get("title") and n.get("summary") and len(n.get("summary", "")) > 50):
            continue
        
        # –°–¢–†–û–ì–ê–Ø –ü–†–û–í–ï–†–ö–ê: –Ω–æ–≤–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        title = n.get("title", "")
        summary = n.get("summary", "")
        text = (title + " " + summary).lower()
        
        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ü–∏—Ñ—Ä—ã (—Ü–µ–Ω—ã, –æ–±—ä–µ–º—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, –¥–∞—Ç—ã)
        has_numbers = bool(re.search(r'\d+', text))
        
        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã (–Ω–µ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã)
        vague_phrases = [
            "limited activity", "no significant", "not mentioned", "under observation",
            "paused", "minimal", "general", "expected", "likely", "potential"
        ]
        has_vague_only = all(phrase in text for phrase in vague_phrases[:2]) and not has_numbers
        
        # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –¥–ª–∏–Ω–∞ summary (–º–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏)
        if len(summary) < 100:
            continue
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not has_numbers and has_vague_only:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {title[:60]}...")
            continue
        
        valid_news.append(n)
    
    if not valid_news:
        print("‚ö†Ô∏è  –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
        return None
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏
    important_keywords = [
        "price", "prices", "export", "import", "demand", "supply",
        "record", "surge", "rise", "fall", "policy", "regulation",
        "mining", "production", "freight", "shipping", "trade",
        "china", "india", "australia", "indonesia", "europe",
        "thermal coal", "coking coal", "benchmark", "index"
    ]
    
    # –ù–∞–¥–µ–∂–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–≤—ã—à–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    premium_sources = [
        "reuters", "bloomberg", "financial times", "ft.com",
        "argus", "platts", "spglobal", "s&p global"
    ]
    
    def priority_score(news):
        score = 0
        
        # 1. –°–≤–µ–∂–µ—Å—Ç—å (—Å–µ–≥–æ–¥–Ω—è = +100, –≤—á–µ—Ä–∞ = +50, –ø–æ–∑–∞–≤—á–µ—Ä–∞ = +25)
        pub_date = news.get("publication_date", "")
        from datetime import datetime
        today = datetime.now().strftime("%Y-%m-%d")
        if pub_date == today:
            score += 100
        elif pub_date:
            score += 50
        
        # 2. –ó–Ω–∞—á–∏–º–æ—Å—Ç—å (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –∏ summary)
        title_lower = news.get("title", "").lower()
        summary_lower = news.get("summary", "").lower()
        text_lower = title_lower + " " + summary_lower
        
        keyword_count = sum(1 for keyword in important_keywords if keyword in text_lower)
        score += keyword_count * 10  # –ö–∞–∂–¥–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ = +10
        
        # 2.1. –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ outlook (–≤—ã—Å–æ–∫–æ —Ü–µ–Ω—è—Ç—Å—è)
        outlook_keywords = ["outlook", "forecast", "forecast", "prediction", "expect", "projection", "trend"]
        if any(keyword in text_lower for keyword in outlook_keywords):
            score += 10  # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–æ–≥–Ω–æ–∑—ã
        
        # 3. –ö–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        source_name = news.get("source_name", "").lower()
        source_url = news.get("source_url", "").lower()
        if any(premium in source_name or premium in source_url for premium in premium_sources):
            score += 50
        
        # 4. –ù–∞–ª–∏—á–∏–µ URL
        if news.get("source_url"):
            score += 30
        
        # 5. –î–ª–∏–Ω–∞ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ—Å—Ç—å summary (–Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è)
        summary_len = len(news.get("summary", ""))
        if 100 <= summary_len <= 500:
            score += min(summary_len // 10, 30)  # –î–æ +30 –∑–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        
        # 6. –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ —Ü–∏—Ñ—Ä –∏ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏ (–ø—Ä–∏–∑–Ω–∞–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏)
        import re
        text_for_numbers = news.get("title", "") + " " + news.get("summary", "")
        numbers_count = len(re.findall(r'\d+', text_for_numbers))
        if numbers_count > 0:
            score += min(numbers_count * 5, 50)  # –î–æ +50 –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ü–∏—Ñ—Ä—ã
        
        # 7. –®—Ç—Ä–∞—Ñ –∑–∞ –æ–±—â–∏–µ —Ñ—Ä–∞–∑—ã –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫–∏
        vague_phrases = ["not mentioned", "no significant", "limited activity", "under observation"]
        text_lower_vague = text_lower
        vague_count = sum(1 for phrase in vague_phrases if phrase in text_lower_vague)
        if vague_count >= 2 and numbers_count == 0:
            score -= 30  # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –æ–±—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
        
        return score
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –±–µ—Ä–µ–º —Å–∞–º—É—é —Ç–æ–ø–æ–≤—É—é
    scored_news = [(priority_score(n), n) for n in valid_news]
    scored_news.sort(reverse=True, key=lambda x: x[0])
    
    best = scored_news[0][1]
    best_score = scored_news[0][0]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º score –≤ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ main.py
    best['_score'] = best_score
    
    print(f"üì∞ –í—ã–±—Ä–∞–Ω–∞ —Å–∞–º–∞—è —Ç–æ–ø–æ–≤–∞—è –Ω–æ–≤–æ—Å—Ç—å (score: {best_score}): {best.get('title', '')[:60]}...")
    if len(scored_news) > 1:
        print(f"   –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(valid_news)} –Ω–æ–≤–æ—Å—Ç–µ–π, –≤—ã–±—Ä–∞–Ω–∞ –ª—É—á—à–∞—è")
    
    return best


def _process_discovery_engine_results(news_list: List[Dict]) -> List[Dict]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã Discovery Engine —á–µ—Ä–µ–∑ Gemini –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö summary.
    
    Args:
        news_list: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ Discovery Engine
        
    Returns:
        –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ summary
    """
    if not news_list:
        return []
    
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        return news_list  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å, –µ—Å–ª–∏ –Ω–µ—Ç API –∫–ª—é—á–∞
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è summary
        model_name = "gemini-2.0-flash-exp"
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={api_key}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
        news_text = "\n\n".join([
            f"Title: {n.get('title', '')}\nURL: {n.get('source_url', '')}\nSnippet: {n.get('summary', '')}"
            for n in news_list[:10]  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º 10 –Ω–æ–≤–æ—Å—Ç–µ–π
        ])
        
        prompt = f"""Process these coal market news from Discovery Engine. For each news item, create a concise summary (2-3 sentences) that includes:
- Specific numbers, prices, volumes, percentages if mentioned
- Concrete facts: company names, port names, countries, exact figures
- NO vague phrases like "limited activity" or "not mentioned"

Return JSON array with improved summaries:
{{
    "news": [{{
        "title": "original title",
        "summary": "improved summary with specific data",
        "source_name": "extracted from URL",
        "source_url": "original URL",
        "publication_date": null
    }}]
}}

News items:
{news_text}"""
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.1,
                "topK": 1,
                "topP": 0.1
            }
        }
        
        response = requests.post(url, json=payload, headers={"Content-Type": "application/json"}, timeout=60)
        response.raise_for_status()
        data = response.json()
        
        if 'candidates' in data and len(data['candidates']) > 0:
            response_text = data['candidates'][0]['content']['parts'][0].get('text', '')
            
            # –ü–∞—Ä—Å–∏–º JSON
            json_start = response_text.find("{")
            json_end = response_text.rfind("}") + 1
            if json_start >= 0 and json_end > json_start:
                json_text = response_text[json_start:json_end]
                processed_data = json.loads(json_text)
                return processed_data.get("news", news_list)
        
        return news_list
        
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Discovery Engine: {e}")
        return news_list

